{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Jan 27 23:24:34 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 450.102.04   Driver Version: 450.102.04   CUDA Version: 11.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  GeForce GTX 106...  Off  | 00000000:01:00.0  On |                  N/A |\n",
      "| N/A   64C    P0    25W /  N/A |    453MiB /  6069MiB |      2%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      1339      G   /usr/lib/xorg/Xorg                 59MiB |\n",
      "|    0   N/A  N/A      2364      G   /usr/lib/xorg/Xorg                179MiB |\n",
      "|    0   N/A  N/A      2679      G   /usr/bin/gnome-shell              123MiB |\n",
      "|    0   N/A  N/A    129112      G   /usr/lib/firefox/firefox            1MiB |\n",
      "|    0   N/A  N/A    165207      G   gnome-control-center                1MiB |\n",
      "|    0   N/A  N/A    580779      G   /usr/lib/firefox/firefox            3MiB |\n",
      "|    0   N/A  N/A    965780      G   ...AAAAAAAAA= --shared-files       71MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "! nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using pytorch Lightning version 1.0.8\n",
      "[INFO] Using pytorch  version 1.7.0\n",
      "[INFO] Using sklearn version 0.23.2\n",
      "[INFO] Using transformers version 3.5.1\n",
      "[INFO] Using pandas 1.1.4\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "\n",
    "import sklearn\n",
    "from sklearn.model_selection import StratifiedKFold, KFold, train_test_split\n",
    "\n",
    "import torch as th \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import pytorch_lightning as pl \n",
    "from pytorch_lightning import seed_everything, Trainer\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping, GPUStatsMonitor\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning.metrics.functional.classification import accuracy, precision\n",
    "from pytorch_lightning.metrics.functional import f1\n",
    "\n",
    "import transformers\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from transformers import get_linear_schedule_with_warmup, AdamW\n",
    "\n",
    "from tqdm import tqdm\n",
    "# Printing stuff\n",
    "\n",
    "print(f\"[INFO] Using pytorch Lightning version {pl.__version__}\")\n",
    "print(f\"[INFO] Using pytorch  version {th.__version__}\")\n",
    "print(f\"[INFO] Using sklearn version {sklearn.__version__}\")\n",
    "print(f\"[INFO] Using transformers version {transformers.__version__}\")\n",
    "print(f\"[INFO] Using pandas {pd.__version__}\")\n",
    "\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class config:\n",
    "    dataset_path = os.path.abspath('../data')\n",
    "    working_dir = os.path.abspath('../')\n",
    "    models_dir = os.path.join(working_dir, 'models')\n",
    "    submissions_dir = os.path.join(working_dir, 'submissions')\n",
    "    logs_dir = os.path.join(working_dir, 'logs')\n",
    "    max_len = 115\n",
    "    base_model = 'distilbert-base-uncased'\n",
    "    train_batch_size = 32\n",
    "    test_batch_size = 16\n",
    "    lr = 3e-4\n",
    "    num_epochs = 10\n",
    "    n_folds=5\n",
    "    seed_value = 2021\n",
    "    num_classes=1\n",
    "    num_workers = os.cpu_count()\n",
    "    test_size = .1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = seed_everything(config.seed_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data viz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(os.path.join(config.dataset_path, 'train.csv'))\n",
    "test_df = pd.read_csv(os.path.join(config.dataset_path, 'test.csv'))\n",
    "submission_df = pd.read_csv(os.path.join(config.submissions_dir, 'sample_submission.csv'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f3ccd0a7d68>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       "  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Created with matplotlib (https://matplotlib.org/) -->\n",
       "<svg height=\"262.19625pt\" version=\"1.1\" viewBox=\"0 0 395.328125 262.19625\" width=\"395.328125pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       " <defs>\n",
       "  <style type=\"text/css\">\n",
       "*{stroke-linecap:butt;stroke-linejoin:round;}\n",
       "  </style>\n",
       " </defs>\n",
       " <g id=\"figure_1\">\n",
       "  <g id=\"patch_1\">\n",
       "   <path d=\"M 0 262.19625 \n",
       "L 395.328125 262.19625 \n",
       "L 395.328125 0 \n",
       "L 0 0 \n",
       "z\n",
       "\" style=\"fill:none;\"/>\n",
       "  </g>\n",
       "  <g id=\"axes_1\">\n",
       "   <g id=\"patch_2\">\n",
       "    <path d=\"M 53.328125 224.64 \n",
       "L 388.128125 224.64 \n",
       "L 388.128125 7.2 \n",
       "L 53.328125 7.2 \n",
       "z\n",
       "\" style=\"fill:#ffffff;\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_3\">\n",
       "    <path clip-path=\"url(#p6356a2903f)\" d=\"M 70.068125 224.64 \n",
       "L 203.988125 224.64 \n",
       "L 203.988125 17.554286 \n",
       "L 70.068125 17.554286 \n",
       "z\n",
       "\" style=\"fill:#3274a1;\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_4\">\n",
       "    <path clip-path=\"url(#p6356a2903f)\" d=\"M 237.468125 224.64 \n",
       "L 371.388125 224.64 \n",
       "L 371.388125 68.634157 \n",
       "L 237.468125 68.634157 \n",
       "z\n",
       "\" style=\"fill:#e1812c;\"/>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_1\">\n",
       "    <g id=\"xtick_1\">\n",
       "     <g id=\"line2d_1\">\n",
       "      <defs>\n",
       "       <path d=\"M 0 0 \n",
       "L 0 3.5 \n",
       "\" id=\"md266a1872c\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"137.028125\" xlink:href=\"#md266a1872c\" y=\"224.64\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_1\">\n",
       "      <!-- 0 -->\n",
       "      <defs>\n",
       "       <path d=\"M 31.78125 66.40625 \n",
       "Q 24.171875 66.40625 20.328125 58.90625 \n",
       "Q 16.5 51.421875 16.5 36.375 \n",
       "Q 16.5 21.390625 20.328125 13.890625 \n",
       "Q 24.171875 6.390625 31.78125 6.390625 \n",
       "Q 39.453125 6.390625 43.28125 13.890625 \n",
       "Q 47.125 21.390625 47.125 36.375 \n",
       "Q 47.125 51.421875 43.28125 58.90625 \n",
       "Q 39.453125 66.40625 31.78125 66.40625 \n",
       "z\n",
       "M 31.78125 74.21875 \n",
       "Q 44.046875 74.21875 50.515625 64.515625 \n",
       "Q 56.984375 54.828125 56.984375 36.375 \n",
       "Q 56.984375 17.96875 50.515625 8.265625 \n",
       "Q 44.046875 -1.421875 31.78125 -1.421875 \n",
       "Q 19.53125 -1.421875 13.0625 8.265625 \n",
       "Q 6.59375 17.96875 6.59375 36.375 \n",
       "Q 6.59375 54.828125 13.0625 64.515625 \n",
       "Q 19.53125 74.21875 31.78125 74.21875 \n",
       "z\n",
       "\" id=\"DejaVuSans-48\"/>\n",
       "      </defs>\n",
       "      <g transform=\"translate(133.846875 239.238437)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-48\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_2\">\n",
       "     <g id=\"line2d_2\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"304.428125\" xlink:href=\"#md266a1872c\" y=\"224.64\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_2\">\n",
       "      <!-- 1 -->\n",
       "      <defs>\n",
       "       <path d=\"M 12.40625 8.296875 \n",
       "L 28.515625 8.296875 \n",
       "L 28.515625 63.921875 \n",
       "L 10.984375 60.40625 \n",
       "L 10.984375 69.390625 \n",
       "L 28.421875 72.90625 \n",
       "L 38.28125 72.90625 \n",
       "L 38.28125 8.296875 \n",
       "L 54.390625 8.296875 \n",
       "L 54.390625 0 \n",
       "L 12.40625 0 \n",
       "z\n",
       "\" id=\"DejaVuSans-49\"/>\n",
       "      </defs>\n",
       "      <g transform=\"translate(301.246875 239.238437)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-49\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_3\">\n",
       "     <!-- target -->\n",
       "     <defs>\n",
       "      <path d=\"M 18.3125 70.21875 \n",
       "L 18.3125 54.6875 \n",
       "L 36.8125 54.6875 \n",
       "L 36.8125 47.703125 \n",
       "L 18.3125 47.703125 \n",
       "L 18.3125 18.015625 \n",
       "Q 18.3125 11.328125 20.140625 9.421875 \n",
       "Q 21.96875 7.515625 27.59375 7.515625 \n",
       "L 36.8125 7.515625 \n",
       "L 36.8125 0 \n",
       "L 27.59375 0 \n",
       "Q 17.1875 0 13.234375 3.875 \n",
       "Q 9.28125 7.765625 9.28125 18.015625 \n",
       "L 9.28125 47.703125 \n",
       "L 2.6875 47.703125 \n",
       "L 2.6875 54.6875 \n",
       "L 9.28125 54.6875 \n",
       "L 9.28125 70.21875 \n",
       "z\n",
       "\" id=\"DejaVuSans-116\"/>\n",
       "      <path d=\"M 34.28125 27.484375 \n",
       "Q 23.390625 27.484375 19.1875 25 \n",
       "Q 14.984375 22.515625 14.984375 16.5 \n",
       "Q 14.984375 11.71875 18.140625 8.90625 \n",
       "Q 21.296875 6.109375 26.703125 6.109375 \n",
       "Q 34.1875 6.109375 38.703125 11.40625 \n",
       "Q 43.21875 16.703125 43.21875 25.484375 \n",
       "L 43.21875 27.484375 \n",
       "z\n",
       "M 52.203125 31.203125 \n",
       "L 52.203125 0 \n",
       "L 43.21875 0 \n",
       "L 43.21875 8.296875 \n",
       "Q 40.140625 3.328125 35.546875 0.953125 \n",
       "Q 30.953125 -1.421875 24.3125 -1.421875 \n",
       "Q 15.921875 -1.421875 10.953125 3.296875 \n",
       "Q 6 8.015625 6 15.921875 \n",
       "Q 6 25.140625 12.171875 29.828125 \n",
       "Q 18.359375 34.515625 30.609375 34.515625 \n",
       "L 43.21875 34.515625 \n",
       "L 43.21875 35.40625 \n",
       "Q 43.21875 41.609375 39.140625 45 \n",
       "Q 35.0625 48.390625 27.6875 48.390625 \n",
       "Q 23 48.390625 18.546875 47.265625 \n",
       "Q 14.109375 46.140625 10.015625 43.890625 \n",
       "L 10.015625 52.203125 \n",
       "Q 14.9375 54.109375 19.578125 55.046875 \n",
       "Q 24.21875 56 28.609375 56 \n",
       "Q 40.484375 56 46.34375 49.84375 \n",
       "Q 52.203125 43.703125 52.203125 31.203125 \n",
       "z\n",
       "\" id=\"DejaVuSans-97\"/>\n",
       "      <path d=\"M 41.109375 46.296875 \n",
       "Q 39.59375 47.171875 37.8125 47.578125 \n",
       "Q 36.03125 48 33.890625 48 \n",
       "Q 26.265625 48 22.1875 43.046875 \n",
       "Q 18.109375 38.09375 18.109375 28.8125 \n",
       "L 18.109375 0 \n",
       "L 9.078125 0 \n",
       "L 9.078125 54.6875 \n",
       "L 18.109375 54.6875 \n",
       "L 18.109375 46.1875 \n",
       "Q 20.953125 51.171875 25.484375 53.578125 \n",
       "Q 30.03125 56 36.53125 56 \n",
       "Q 37.453125 56 38.578125 55.875 \n",
       "Q 39.703125 55.765625 41.0625 55.515625 \n",
       "z\n",
       "\" id=\"DejaVuSans-114\"/>\n",
       "      <path d=\"M 45.40625 27.984375 \n",
       "Q 45.40625 37.75 41.375 43.109375 \n",
       "Q 37.359375 48.484375 30.078125 48.484375 \n",
       "Q 22.859375 48.484375 18.828125 43.109375 \n",
       "Q 14.796875 37.75 14.796875 27.984375 \n",
       "Q 14.796875 18.265625 18.828125 12.890625 \n",
       "Q 22.859375 7.515625 30.078125 7.515625 \n",
       "Q 37.359375 7.515625 41.375 12.890625 \n",
       "Q 45.40625 18.265625 45.40625 27.984375 \n",
       "z\n",
       "M 54.390625 6.78125 \n",
       "Q 54.390625 -7.171875 48.1875 -13.984375 \n",
       "Q 42 -20.796875 29.203125 -20.796875 \n",
       "Q 24.46875 -20.796875 20.265625 -20.09375 \n",
       "Q 16.0625 -19.390625 12.109375 -17.921875 \n",
       "L 12.109375 -9.1875 \n",
       "Q 16.0625 -11.328125 19.921875 -12.34375 \n",
       "Q 23.78125 -13.375 27.78125 -13.375 \n",
       "Q 36.625 -13.375 41.015625 -8.765625 \n",
       "Q 45.40625 -4.15625 45.40625 5.171875 \n",
       "L 45.40625 9.625 \n",
       "Q 42.625 4.78125 38.28125 2.390625 \n",
       "Q 33.9375 0 27.875 0 \n",
       "Q 17.828125 0 11.671875 7.65625 \n",
       "Q 5.515625 15.328125 5.515625 27.984375 \n",
       "Q 5.515625 40.671875 11.671875 48.328125 \n",
       "Q 17.828125 56 27.875 56 \n",
       "Q 33.9375 56 38.28125 53.609375 \n",
       "Q 42.625 51.21875 45.40625 46.390625 \n",
       "L 45.40625 54.6875 \n",
       "L 54.390625 54.6875 \n",
       "z\n",
       "\" id=\"DejaVuSans-103\"/>\n",
       "      <path d=\"M 56.203125 29.59375 \n",
       "L 56.203125 25.203125 \n",
       "L 14.890625 25.203125 \n",
       "Q 15.484375 15.921875 20.484375 11.0625 \n",
       "Q 25.484375 6.203125 34.421875 6.203125 \n",
       "Q 39.59375 6.203125 44.453125 7.46875 \n",
       "Q 49.3125 8.734375 54.109375 11.28125 \n",
       "L 54.109375 2.78125 \n",
       "Q 49.265625 0.734375 44.1875 -0.34375 \n",
       "Q 39.109375 -1.421875 33.890625 -1.421875 \n",
       "Q 20.796875 -1.421875 13.15625 6.1875 \n",
       "Q 5.515625 13.8125 5.515625 26.8125 \n",
       "Q 5.515625 40.234375 12.765625 48.109375 \n",
       "Q 20.015625 56 32.328125 56 \n",
       "Q 43.359375 56 49.78125 48.890625 \n",
       "Q 56.203125 41.796875 56.203125 29.59375 \n",
       "z\n",
       "M 47.21875 32.234375 \n",
       "Q 47.125 39.59375 43.09375 43.984375 \n",
       "Q 39.0625 48.390625 32.421875 48.390625 \n",
       "Q 24.90625 48.390625 20.390625 44.140625 \n",
       "Q 15.875 39.890625 15.1875 32.171875 \n",
       "z\n",
       "\" id=\"DejaVuSans-101\"/>\n",
       "     </defs>\n",
       "     <g transform=\"translate(205.438281 252.916562)scale(0.1 -0.1)\">\n",
       "      <use xlink:href=\"#DejaVuSans-116\"/>\n",
       "      <use x=\"39.208984\" xlink:href=\"#DejaVuSans-97\"/>\n",
       "      <use x=\"100.488281\" xlink:href=\"#DejaVuSans-114\"/>\n",
       "      <use x=\"141.585938\" xlink:href=\"#DejaVuSans-103\"/>\n",
       "      <use x=\"205.0625\" xlink:href=\"#DejaVuSans-101\"/>\n",
       "      <use x=\"266.585938\" xlink:href=\"#DejaVuSans-116\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_2\">\n",
       "    <g id=\"ytick_1\">\n",
       "     <g id=\"line2d_3\">\n",
       "      <defs>\n",
       "       <path d=\"M 0 0 \n",
       "L -3.5 0 \n",
       "\" id=\"m5affb3e4ae\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"53.328125\" xlink:href=\"#m5affb3e4ae\" y=\"224.64\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_4\">\n",
       "      <!-- 0 -->\n",
       "      <g transform=\"translate(39.965625 228.439219)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-48\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_2\">\n",
       "     <g id=\"line2d_4\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"53.328125\" xlink:href=\"#m5affb3e4ae\" y=\"176.946376\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_5\">\n",
       "      <!-- 1000 -->\n",
       "      <g transform=\"translate(20.878125 180.745595)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-49\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_3\">\n",
       "     <g id=\"line2d_5\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"53.328125\" xlink:href=\"#m5affb3e4ae\" y=\"129.252753\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_6\">\n",
       "      <!-- 2000 -->\n",
       "      <defs>\n",
       "       <path d=\"M 19.1875 8.296875 \n",
       "L 53.609375 8.296875 \n",
       "L 53.609375 0 \n",
       "L 7.328125 0 \n",
       "L 7.328125 8.296875 \n",
       "Q 12.9375 14.109375 22.625 23.890625 \n",
       "Q 32.328125 33.6875 34.8125 36.53125 \n",
       "Q 39.546875 41.84375 41.421875 45.53125 \n",
       "Q 43.3125 49.21875 43.3125 52.78125 \n",
       "Q 43.3125 58.59375 39.234375 62.25 \n",
       "Q 35.15625 65.921875 28.609375 65.921875 \n",
       "Q 23.96875 65.921875 18.8125 64.3125 \n",
       "Q 13.671875 62.703125 7.8125 59.421875 \n",
       "L 7.8125 69.390625 \n",
       "Q 13.765625 71.78125 18.9375 73 \n",
       "Q 24.125 74.21875 28.421875 74.21875 \n",
       "Q 39.75 74.21875 46.484375 68.546875 \n",
       "Q 53.21875 62.890625 53.21875 53.421875 \n",
       "Q 53.21875 48.921875 51.53125 44.890625 \n",
       "Q 49.859375 40.875 45.40625 35.40625 \n",
       "Q 44.1875 33.984375 37.640625 27.21875 \n",
       "Q 31.109375 20.453125 19.1875 8.296875 \n",
       "z\n",
       "\" id=\"DejaVuSans-50\"/>\n",
       "      </defs>\n",
       "      <g transform=\"translate(20.878125 133.051971)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-50\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_4\">\n",
       "     <g id=\"line2d_6\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"53.328125\" xlink:href=\"#m5affb3e4ae\" y=\"81.559129\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_7\">\n",
       "      <!-- 3000 -->\n",
       "      <defs>\n",
       "       <path d=\"M 40.578125 39.3125 \n",
       "Q 47.65625 37.796875 51.625 33 \n",
       "Q 55.609375 28.21875 55.609375 21.1875 \n",
       "Q 55.609375 10.40625 48.1875 4.484375 \n",
       "Q 40.765625 -1.421875 27.09375 -1.421875 \n",
       "Q 22.515625 -1.421875 17.65625 -0.515625 \n",
       "Q 12.796875 0.390625 7.625 2.203125 \n",
       "L 7.625 11.71875 \n",
       "Q 11.71875 9.328125 16.59375 8.109375 \n",
       "Q 21.484375 6.890625 26.8125 6.890625 \n",
       "Q 36.078125 6.890625 40.9375 10.546875 \n",
       "Q 45.796875 14.203125 45.796875 21.1875 \n",
       "Q 45.796875 27.640625 41.28125 31.265625 \n",
       "Q 36.765625 34.90625 28.71875 34.90625 \n",
       "L 20.21875 34.90625 \n",
       "L 20.21875 43.015625 \n",
       "L 29.109375 43.015625 \n",
       "Q 36.375 43.015625 40.234375 45.921875 \n",
       "Q 44.09375 48.828125 44.09375 54.296875 \n",
       "Q 44.09375 59.90625 40.109375 62.90625 \n",
       "Q 36.140625 65.921875 28.71875 65.921875 \n",
       "Q 24.65625 65.921875 20.015625 65.03125 \n",
       "Q 15.375 64.15625 9.8125 62.3125 \n",
       "L 9.8125 71.09375 \n",
       "Q 15.4375 72.65625 20.34375 73.4375 \n",
       "Q 25.25 74.21875 29.59375 74.21875 \n",
       "Q 40.828125 74.21875 47.359375 69.109375 \n",
       "Q 53.90625 64.015625 53.90625 55.328125 \n",
       "Q 53.90625 49.265625 50.4375 45.09375 \n",
       "Q 46.96875 40.921875 40.578125 39.3125 \n",
       "z\n",
       "\" id=\"DejaVuSans-51\"/>\n",
       "      </defs>\n",
       "      <g transform=\"translate(20.878125 85.358348)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-51\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_5\">\n",
       "     <g id=\"line2d_7\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"53.328125\" xlink:href=\"#m5affb3e4ae\" y=\"33.865505\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_8\">\n",
       "      <!-- 4000 -->\n",
       "      <defs>\n",
       "       <path d=\"M 37.796875 64.3125 \n",
       "L 12.890625 25.390625 \n",
       "L 37.796875 25.390625 \n",
       "z\n",
       "M 35.203125 72.90625 \n",
       "L 47.609375 72.90625 \n",
       "L 47.609375 25.390625 \n",
       "L 58.015625 25.390625 \n",
       "L 58.015625 17.1875 \n",
       "L 47.609375 17.1875 \n",
       "L 47.609375 0 \n",
       "L 37.796875 0 \n",
       "L 37.796875 17.1875 \n",
       "L 4.890625 17.1875 \n",
       "L 4.890625 26.703125 \n",
       "z\n",
       "\" id=\"DejaVuSans-52\"/>\n",
       "      </defs>\n",
       "      <g transform=\"translate(20.878125 37.664724)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-52\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_9\">\n",
       "     <!-- count -->\n",
       "     <defs>\n",
       "      <path d=\"M 48.78125 52.59375 \n",
       "L 48.78125 44.1875 \n",
       "Q 44.96875 46.296875 41.140625 47.34375 \n",
       "Q 37.3125 48.390625 33.40625 48.390625 \n",
       "Q 24.65625 48.390625 19.8125 42.84375 \n",
       "Q 14.984375 37.3125 14.984375 27.296875 \n",
       "Q 14.984375 17.28125 19.8125 11.734375 \n",
       "Q 24.65625 6.203125 33.40625 6.203125 \n",
       "Q 37.3125 6.203125 41.140625 7.25 \n",
       "Q 44.96875 8.296875 48.78125 10.40625 \n",
       "L 48.78125 2.09375 \n",
       "Q 45.015625 0.34375 40.984375 -0.53125 \n",
       "Q 36.96875 -1.421875 32.421875 -1.421875 \n",
       "Q 20.0625 -1.421875 12.78125 6.34375 \n",
       "Q 5.515625 14.109375 5.515625 27.296875 \n",
       "Q 5.515625 40.671875 12.859375 48.328125 \n",
       "Q 20.21875 56 33.015625 56 \n",
       "Q 37.15625 56 41.109375 55.140625 \n",
       "Q 45.0625 54.296875 48.78125 52.59375 \n",
       "z\n",
       "\" id=\"DejaVuSans-99\"/>\n",
       "      <path d=\"M 30.609375 48.390625 \n",
       "Q 23.390625 48.390625 19.1875 42.75 \n",
       "Q 14.984375 37.109375 14.984375 27.296875 \n",
       "Q 14.984375 17.484375 19.15625 11.84375 \n",
       "Q 23.34375 6.203125 30.609375 6.203125 \n",
       "Q 37.796875 6.203125 41.984375 11.859375 \n",
       "Q 46.1875 17.53125 46.1875 27.296875 \n",
       "Q 46.1875 37.015625 41.984375 42.703125 \n",
       "Q 37.796875 48.390625 30.609375 48.390625 \n",
       "z\n",
       "M 30.609375 56 \n",
       "Q 42.328125 56 49.015625 48.375 \n",
       "Q 55.71875 40.765625 55.71875 27.296875 \n",
       "Q 55.71875 13.875 49.015625 6.21875 \n",
       "Q 42.328125 -1.421875 30.609375 -1.421875 \n",
       "Q 18.84375 -1.421875 12.171875 6.21875 \n",
       "Q 5.515625 13.875 5.515625 27.296875 \n",
       "Q 5.515625 40.765625 12.171875 48.375 \n",
       "Q 18.84375 56 30.609375 56 \n",
       "z\n",
       "\" id=\"DejaVuSans-111\"/>\n",
       "      <path d=\"M 8.5 21.578125 \n",
       "L 8.5 54.6875 \n",
       "L 17.484375 54.6875 \n",
       "L 17.484375 21.921875 \n",
       "Q 17.484375 14.15625 20.5 10.265625 \n",
       "Q 23.53125 6.390625 29.59375 6.390625 \n",
       "Q 36.859375 6.390625 41.078125 11.03125 \n",
       "Q 45.3125 15.671875 45.3125 23.6875 \n",
       "L 45.3125 54.6875 \n",
       "L 54.296875 54.6875 \n",
       "L 54.296875 0 \n",
       "L 45.3125 0 \n",
       "L 45.3125 8.40625 \n",
       "Q 42.046875 3.421875 37.71875 1 \n",
       "Q 33.40625 -1.421875 27.6875 -1.421875 \n",
       "Q 18.265625 -1.421875 13.375 4.4375 \n",
       "Q 8.5 10.296875 8.5 21.578125 \n",
       "z\n",
       "M 31.109375 56 \n",
       "z\n",
       "\" id=\"DejaVuSans-117\"/>\n",
       "      <path d=\"M 54.890625 33.015625 \n",
       "L 54.890625 0 \n",
       "L 45.90625 0 \n",
       "L 45.90625 32.71875 \n",
       "Q 45.90625 40.484375 42.875 44.328125 \n",
       "Q 39.84375 48.1875 33.796875 48.1875 \n",
       "Q 26.515625 48.1875 22.3125 43.546875 \n",
       "Q 18.109375 38.921875 18.109375 30.90625 \n",
       "L 18.109375 0 \n",
       "L 9.078125 0 \n",
       "L 9.078125 54.6875 \n",
       "L 18.109375 54.6875 \n",
       "L 18.109375 46.1875 \n",
       "Q 21.34375 51.125 25.703125 53.5625 \n",
       "Q 30.078125 56 35.796875 56 \n",
       "Q 45.21875 56 50.046875 50.171875 \n",
       "Q 54.890625 44.34375 54.890625 33.015625 \n",
       "z\n",
       "\" id=\"DejaVuSans-110\"/>\n",
       "     </defs>\n",
       "     <g transform=\"translate(14.798438 130.02625)rotate(-90)scale(0.1 -0.1)\">\n",
       "      <use xlink:href=\"#DejaVuSans-99\"/>\n",
       "      <use x=\"54.980469\" xlink:href=\"#DejaVuSans-111\"/>\n",
       "      <use x=\"116.162109\" xlink:href=\"#DejaVuSans-117\"/>\n",
       "      <use x=\"179.541016\" xlink:href=\"#DejaVuSans-110\"/>\n",
       "      <use x=\"242.919922\" xlink:href=\"#DejaVuSans-116\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"patch_5\">\n",
       "    <path d=\"M 53.328125 224.64 \n",
       "L 53.328125 7.2 \n",
       "\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_6\">\n",
       "    <path d=\"M 388.128125 224.64 \n",
       "L 388.128125 7.2 \n",
       "\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_7\">\n",
       "    <path d=\"M 53.328125 224.64 \n",
       "L 388.128125 224.64 \n",
       "\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_8\">\n",
       "    <path d=\"M 53.328125 7.2 \n",
       "L 388.128125 7.2 \n",
       "\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n",
       "   </g>\n",
       "  </g>\n",
       " </g>\n",
       " <defs>\n",
       "  <clipPath id=\"p6356a2903f\">\n",
       "   <rect height=\"217.44\" width=\"334.8\" x=\"53.328125\" y=\"7.2\"/>\n",
       "  </clipPath>\n",
       " </defs>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(data=train_df, x=\"target\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>Text_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "      <td>133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  Text_len  \n",
       "0       1        69  \n",
       "1       1        38  \n",
       "2       1       133  \n",
       "3       1        65  \n",
       "4       1        88  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['Text_len'] = train_df['text'].apply(lambda txt: len(txt))\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "      <th>Text_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>7613.000000</td>\n",
       "      <td>7613.00000</td>\n",
       "      <td>7613.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>5441.934848</td>\n",
       "      <td>0.42966</td>\n",
       "      <td>101.037436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3137.116090</td>\n",
       "      <td>0.49506</td>\n",
       "      <td>33.781325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2734.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>78.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>5408.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>107.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>8146.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>133.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>10873.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>157.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id      target     Text_len\n",
       "count   7613.000000  7613.00000  7613.000000\n",
       "mean    5441.934848     0.42966   101.037436\n",
       "std     3137.116090     0.49506    33.781325\n",
       "min        1.000000     0.00000     7.000000\n",
       "25%     2734.000000     0.00000    78.000000\n",
       "50%     5408.000000     0.00000   107.000000\n",
       "75%     8146.000000     1.00000   133.000000\n",
       "max    10873.000000     1.00000   157.000000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7613, 6)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_folds(df:pd.DataFrame, k=5, stratified=False, save=False):\n",
    "    df['fold'] = 0\n",
    "    if stratified:\n",
    "        cv = StratifiedKFold(n_splits=k)\n",
    "    else:\n",
    "        cv = KFold(n_splits=k)\n",
    "        \n",
    "    for index, (train_idx, test_idx) in enumerate(tqdm(cv.split(df), position=0, desc='Making folds for corss validation')):\n",
    "        # print(index, train, test)\n",
    "        df.loc[test_idx, 'fold'] = index\n",
    "    df = df.sample(frac=1).reset_index(drop=True)\n",
    "    if save :\n",
    "        fn = os.path.join(config.working_dir, f'dataset_{k}_folds.csv')\n",
    "        df.to_csv(fn)\n",
    "        print(f'[INFO] Dataset saved to working directory as {fn}')\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Making folds for corss validation: 10it [00:00, 799.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Dataset saved to working directory as /kaggle/working/dataset_10_folds.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "_ = make_folds(df=train_df, k=10, stratified=False, save=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f5f01ff7290>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(data=train_df, x='fold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1296x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(18, 6))\n",
    "sns.distplot(a=train_df['Text_len'])\n",
    "plt.title('Text lengths distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset & Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DisasterDataset(Dataset):\n",
    "    def __init__(self, df:pd.DataFrame, max_len:int, task='train'):\n",
    "        super(DisasterDataset, self).__init__()\n",
    "        self.task = task \n",
    "        self.max_len = max_len \n",
    "        self.df = df\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(config.base_model)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        text = self.df.iloc[index].text\n",
    "        code = self.tokenizer.encode_plus(text=text, \n",
    "                                          return_tensors='pt', \n",
    "                                          truncation=True,\n",
    "                                          max_length= self.max_len,\n",
    "                                          padding='max_length'\n",
    "                                         )\n",
    "        \n",
    "        sample = {\n",
    "            'str' : text,\n",
    "            'ids': code['input_ids'], \n",
    "            'attention_mask':code['attention_mask']\n",
    "        }\n",
    "        \n",
    "        if self.task == \"train\":\n",
    "            target = self.df.iloc[index].target\n",
    "            sample.update({\n",
    "                'targets': th.tensor([target], dtype=th.float)\n",
    "            })\n",
    "            \n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, validation_dataset = train_test_split(train_df, random_state=config.seed_value)\n",
    "\n",
    "# using our custom dataset\n",
    "train_ds = DisasterDataset(df=train_dataset, max_len = config.max_len, task='train')\n",
    "val_ds = DisasterDataset(df=validation_dataset, max_len = config.max_len, task='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5709, 1904)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_ds), len(val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'str': '@CalFreedomMom @steph93065 not to mention a major contributor to the annihilation of Israel',\n",
       " 'ids': tensor([[  101,  1030, 19134,  9910,  9527,  5302,  2213,  1030,  3357,  2232,\n",
       "           2683, 14142, 26187,  2025,  2000,  5254,  1037,  2350, 12130,  2000,\n",
       "           1996,  5754, 19190, 29545,  1997,  3956,   102,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0]]),\n",
       " 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]),\n",
       " 'targets': tensor([1.])}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_ds[15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloaders\n",
    "train_dl = DataLoader(dataset=train_ds, \n",
    "                      batch_size=config.train_batch_size, \n",
    "                      shuffle=True)\n",
    "\n",
    "\n",
    "val_dl = DataLoader(dataset=val_ds, \n",
    "                      batch_size=config.test_batch_size, \n",
    "                      shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 115]) torch.Size([16, 1])\n"
     ]
    }
   ],
   "source": [
    "for data in val_dl:\n",
    "    print(data['ids'].squeeze(1).shape, data['targets'].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "\n",
    "* Create Lightning module\n",
    "* use bert pretrained model as encoder\n",
    "* adding decoder layer Linear(encoder_out, num_classes) -> (768, 2)\n",
    "* compute F1 score (and in addition loss and accuracy)\n",
    "* use optimizer to optimize metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertModel(\n",
       "  (embeddings): Embeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (transformer): Transformer(\n",
       "    (layer): ModuleList(\n",
       "      (0): TransformerBlock(\n",
       "        (attention): MultiHeadSelfAttention(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (ffn): FFN(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "        (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (1): TransformerBlock(\n",
       "        (attention): MultiHeadSelfAttention(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (ffn): FFN(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "        (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (2): TransformerBlock(\n",
       "        (attention): MultiHeadSelfAttention(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (ffn): FFN(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "        (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (3): TransformerBlock(\n",
       "        (attention): MultiHeadSelfAttention(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (ffn): FFN(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "        (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (4): TransformerBlock(\n",
       "        (attention): MultiHeadSelfAttention(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (ffn): FFN(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "        (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (5): TransformerBlock(\n",
       "        (attention): MultiHeadSelfAttention(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (ffn): FFN(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "        (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model = AutoModel.from_pretrained(config.base_model)\n",
    "base_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DisasterTweetsClassifier(pl.LightningModule):\n",
    "    def __init__(self, num_epochs:int, lr:float, freeze:bool, num_classes:int, dropout:float, len_train_ds:int, steps_per_epoch=None):\n",
    "        super(DisasterTweetsClassifier, self).__init__()\n",
    "        self.num_epochs = num_epochs\n",
    "        if steps_per_epoch is None:\n",
    "            self.steps_per_epoch = len_train_ds //config.train_batch_size\n",
    "        else:\n",
    "            self.steps_per_epoch = steps_per_epoch\n",
    "            \n",
    "        self.lr = lr\n",
    "        self.freeze = freeze\n",
    "        \n",
    "        # model architecture\n",
    "        self.dropout = dropout\n",
    "        # encoder part\n",
    "        self.encoder = AutoModel.from_pretrained(config.base_model)\n",
    "        if self.freeze:\n",
    "            # self.encoder.freeze()\n",
    "            for p in self.encoder.parameters():\n",
    "                p.requires_grad = False\n",
    "        try:\n",
    "            self.num_ftrs = self.encoder.transformer.layer[5].ffn.lin2.out_features\n",
    "        except:\n",
    "            self.num_ftrs = 768 # default output dim for all bert last layer\n",
    "        \n",
    "        self.dropout = nn.Dropout(p=self.dropout)\n",
    "        self.num_classes = num_classes\n",
    "    \n",
    "        # classifier part\n",
    "        self.classifier = nn.Linear(in_features=self.num_ftrs, out_features=self.num_classes)\n",
    "        if self.num_classes > 2:\n",
    "            self.loss_fn = nn.NLLLoss()\n",
    "        else:\n",
    "            self.loss_fn = nn.BCELoss()\n",
    "\n",
    "        # optimization stuff\n",
    "        self.warmup_steps = self.steps_per_epoch // 3\n",
    "        self.total_steps = self.steps_per_epoch * self.num_epochs - self.warmup_steps\n",
    "        \n",
    "\n",
    "        \n",
    "    \n",
    "        \n",
    "    def forward(self, ids, attention_mask):\n",
    "        out = self.encoder(input_ids=ids, attention_mask=attention_mask)\n",
    "        last_hidden_state = out[0]\n",
    "        out = self.dropout(last_hidden_state[:, 0])\n",
    "        out = self.classifier(out)\n",
    "        \n",
    "        if self.num_classes > 2:\n",
    "            out = th.softmax(out, dim=1)\n",
    "        else :\n",
    "            out = th.sigmoid(out)\n",
    "    \n",
    "        return out\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "       \n",
    "        if self.freeze:\n",
    "            opt = AdamW(self.classifier.parameters(),\n",
    "                          lr = self.lr, \n",
    "                          eps = 1e-8, \n",
    "                          weight_decay = 0.01\n",
    "                )\n",
    "        else:\n",
    "            opt = AdamW(self.parameters(),\n",
    "              lr = self.lr, \n",
    "              eps = 1e-8, \n",
    "              weight_decay = 0.01\n",
    "            )        \n",
    "\n",
    "\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "                    optimizer = opt, \n",
    "                    num_warmup_steps = self.warmup_steps,\n",
    "                    num_training_steps = self.total_steps\n",
    "        )\n",
    "\n",
    "        return [opt], [scheduler]\n",
    "    \n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        ids , attention_mask, targets = batch['ids'].squeeze(1), batch['attention_mask'].squeeze(1), batch['targets']\n",
    "        \n",
    "        preds = self(ids=ids, attention_mask=attention_mask)\n",
    "        \n",
    "        # loss\n",
    "        loss = self.loss_fn(preds, targets)\n",
    "        \n",
    "        # f1\n",
    "        f1_score = f1(preds = preds, target=targets, num_classes=self.num_classes)\n",
    "        \n",
    "        # precision\n",
    "        precision_score = precision(pred=preds, target=targets, num_classes=self.num_classes)\n",
    "        \n",
    "        acc = accuracy(pred=preds, target=targets, num_classes=self.num_classes)\n",
    "        \n",
    "        #tb_logs = {'train_loss': loss, 'f1-score_train': f1_score, 'precision_train':precision_score}\n",
    "        \n",
    "        self.log('f1_train', f1_score, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        \n",
    "        return {'loss' : loss, \n",
    "                'accuracy': acc,\n",
    "                \"predictions\": preds, \n",
    "                'targets':targets, \n",
    "                \"precision\":precision_score, \n",
    "                \"f1_score\":f1_score}\n",
    "    \n",
    "    \n",
    "    \n",
    "    def training_epoch_end(self,outputs):\n",
    "        #  the function is called after every epoch is completed\n",
    "\n",
    "        # calculating average loss  \n",
    "        avg_loss = th.stack([x['loss'] for x in outputs]).mean()\n",
    "        # acc\n",
    "        avg_acc = th.stack([x['accuracy'] for x in outputs]).mean()\n",
    "        \n",
    "        # calculating average precision  \n",
    "        avg_precision = th.stack([x['precision'] for x in outputs]).mean()\n",
    "        \n",
    "        # calculating average f1  \n",
    "        avg_f1 = th.stack([x['f1_score'] for x in outputs]).mean()\n",
    "                \n",
    "        # logging using tensorboard logger\n",
    "        self.logger.experiment.add_scalar(\"Loss/Train\",\n",
    "                                            avg_loss,\n",
    "                                            self.current_epoch)\n",
    "        \n",
    "        self.logger.experiment.add_scalar(\"Accuracy/Train\",\n",
    "                                            avg_acc,\n",
    "                                            self.current_epoch)\n",
    "\n",
    "        self.logger.experiment.add_scalar(\"F1/Train\",\n",
    "                                            avg_f1,\n",
    "                                            self.current_epoch)\n",
    "        \n",
    "        self.logger.experiment.add_scalar(\"Precision/Train\",\n",
    "                                            avg_precision,\n",
    "                                            self.current_epoch)\n",
    "\n",
    "\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        ids , attention_mask, targets = batch['ids'].squeeze(1), batch['attention_mask'].squeeze(1), batch['targets']\n",
    "        \n",
    "        preds = self(ids=ids, attention_mask=attention_mask)\n",
    "        \n",
    "        # loss\n",
    "        loss = self.loss_fn(preds.detach().cpu(), targets.detach().cpu())\n",
    "        \n",
    "        # f1\n",
    "        f1_score = f1(preds = preds.detach().cpu(), target=targets.detach().cpu(), num_classes=self.num_classes)\n",
    "        \n",
    "        # precision\n",
    "        precision_score = precision(pred=preds.detach().cpu(), target=targets.detach().cpu(), num_classes=self.num_classes)\n",
    "        \n",
    "        acc = accuracy(pred=preds, target=targets, num_classes=self.num_classes)\n",
    "        \n",
    "        #tb_logs = {'train_loss': loss, 'f1-score_train': f1_score, 'precision_train':precision_score}\n",
    "        \n",
    "        self.log('f1_val', f1_score, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "        \n",
    "        return {'loss' : loss, \n",
    "                'accuracy': acc,\n",
    "                \"predictions\": preds, \n",
    "                'targets':targets, \n",
    "                \"precision\":precision_score, \n",
    "                \"f1_score\":f1_score}\n",
    "    \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        #  the function is called after every epoch is completed\n",
    "\n",
    "        # calculating average loss  \n",
    "        avg_loss = th.stack([x['loss'] for x in outputs]).mean()\n",
    "        # acc\n",
    "        avg_acc = th.stack([x['accuracy'] for x in outputs]).mean()\n",
    "        \n",
    "        # calculating average precision  \n",
    "        avg_precision = th.stack([x['precision'] for x in outputs]).mean()\n",
    "        \n",
    "        # calculating average f1  \n",
    "        avg_f1 = th.stack([x['f1_score'] for x in outputs]).mean()\n",
    "                \n",
    "        # logging using tensorboard logger\n",
    "        self.logger.experiment.add_scalar(\"Loss/Validation\",\n",
    "                                            avg_loss,\n",
    "                                            self.current_epoch)\n",
    "        \n",
    "        self.logger.experiment.add_scalar(\"Accuracy/Validation\",\n",
    "                                            avg_acc,\n",
    "                                            self.current_epoch)\n",
    "\n",
    "        self.logger.experiment.add_scalar(\"F1/Validation\",\n",
    "                                            avg_f1,\n",
    "                                            self.current_epoch)\n",
    "        \n",
    "        self.logger.experiment.add_scalar(\"Precision/Validation\",\n",
    "                                            avg_precision,\n",
    "                                            self.current_epoch)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model = DisasterTweetsClassifier(num_epochs=config.num_epochs,\n",
    "                                 lr=config.lr, \n",
    "                                 freeze=False,\n",
    "                                 num_classes=config.num_classes,\n",
    "                                 dropout = .3,\n",
    "                                 len_train_ds=len(train_ds)).to('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_kg_hide-output": true
   },
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model.total_steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model.warmup_steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for data in val_dl:\n",
    "    out = model(ids = data['ids'].squeeze(1).to('cuda'), attention_mask=data['attention_mask'].squeeze(1).to('cuda'))\n",
    "    loss = model.loss_fn(out, data['targets'].to('cuda'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loss.detach().cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross validation \n",
    "def run_fold(fold_num,num_epochs, dataset):\n",
    "    th.cuda.empty_cache()\n",
    "    # get data\n",
    "    \n",
    "    train_dataset = dataset[dataset.fold!=fold_num].reset_index(drop=True)\n",
    "    validation_dataset = dataset[dataset.fold==fold_num].reset_index(drop=True)\n",
    "\n",
    "    # using our custom dataset\n",
    "    train_ds = DisasterDataset(df=train_dataset, max_len = config.max_len, task='train')\n",
    "    val_ds = DisasterDataset(df=validation_dataset, max_len = config.max_len, task='train') \n",
    "    \n",
    "    \n",
    "    # dataloaders\n",
    "    train_dl = DataLoader(dataset=train_ds, \n",
    "                          batch_size=config.train_batch_size, \n",
    "                          shuffle=True)\n",
    "\n",
    "\n",
    "    val_dl = DataLoader(dataset=val_ds, \n",
    "                          batch_size=config.test_batch_size, \n",
    "                          shuffle=False)\n",
    "\n",
    "\n",
    "\n",
    "    #create model\n",
    "    net = DisasterTweetsClassifier(num_epochs=num_epochs,\n",
    "                                 lr=config.lr, \n",
    "                                 freeze=False,\n",
    "                                 num_classes=config.num_classes,\n",
    "                                 dropout = .3,\n",
    "                                 len_train_ds=len(train_ds))\n",
    "\n",
    "    \n",
    "    del train_ds\n",
    "    del val_ds\n",
    "    del train_dataset\n",
    "    del validation_dataset\n",
    "    \n",
    "    # callbacks & loggers\n",
    "    gpu_stats = GPUStatsMonitor(temperature=True)\n",
    "    model_ckpt = ModelCheckpoint(filename=f'disasterTweetsClassifier-fold-{fold_num}', \n",
    "                                 monitor='f1_val',\n",
    "                                 mode='max',\n",
    "                                 dirpath=config.models_dir, \n",
    "                                )\n",
    "    es = EarlyStopping(monitor='f1_val', \n",
    "                       patience=5, \n",
    "                       mode='max')\n",
    "    \n",
    "    tb_logger = TensorBoardLogger(\n",
    "        save_dir=config.logs_dir,\n",
    "        name=f'disasterTweetsClassifier-fold-{fold_num}'\n",
    "    )\n",
    "\n",
    "    Callbacks = [gpu_stats, model_ckpt, es]\n",
    "\n",
    "\n",
    "    # create trainer\n",
    "    trainer = Trainer(gpus=-1, max_epochs=num_epochs, callbacks=Callbacks, logger=tb_logger)\n",
    "    # fit model \n",
    "    \n",
    "\n",
    "    \n",
    "    trainer.fit(model=net, train_dataloader=train_dl, val_dataloaders=val_dl)\n",
    "    \n",
    "    del net\n",
    "    # return metrics\n",
    "    return trainer.logged_metrics\n",
    "\n",
    "\n",
    "# one way training\n",
    "def run(num_epochs, dataset):\n",
    "    th.cuda.empty_cache()\n",
    "\n",
    "    # get data\n",
    "    train_dataset, validation_dataset = train_test_split(dataset, \n",
    "                                                         random_state=config.seed_value, \n",
    "                                                         test_size=config.test_size)\n",
    "\n",
    "    # using our custom dataset\n",
    "    train_ds = DisasterDataset(df=train_dataset, max_len = config.max_len, task='train')\n",
    "    val_ds = DisasterDataset(df=validation_dataset, max_len = config.max_len, task='train')    \n",
    "    \n",
    "    \n",
    "    # dataloaders\n",
    "    train_dl = DataLoader(dataset=train_ds, \n",
    "                          batch_size=config.train_batch_size, \n",
    "                          shuffle=True)\n",
    "\n",
    "\n",
    "    val_dl = DataLoader(dataset=val_ds, \n",
    "                          batch_size=config.test_batch_size, \n",
    "                          shuffle=False)\n",
    "\n",
    "\n",
    "    net = DisasterTweetsClassifier(num_epochs=num_epochs,\n",
    "                                 lr=config.lr, \n",
    "                                 freeze=False,\n",
    "                                 num_classes=config.num_classes,\n",
    "                                 dropout = .3,\n",
    "                                 len_train_ds=len(train_ds))\n",
    "    \n",
    "    del train_ds\n",
    "    del val_ds\n",
    "    # callbacks & loggers\n",
    "    gpu_stats = GPUStatsMonitor(temperature=True)\n",
    "    model_ckpt = ModelCheckpoint(filename=f'disasterTweetsClassifier-all', \n",
    "                                 monitor='f1_val',\n",
    "                                 mode='max',\n",
    "                                 dirpath=config.models_dir, \n",
    "                                )\n",
    "    es = EarlyStopping(monitor='f1_val', \n",
    "                       patience=5, \n",
    "                       mode='max')\n",
    "    \n",
    "    tb_logger = TensorBoardLogger(\n",
    "        save_dir=config.logs_dir,\n",
    "        name=f'disasterTweetsClassifier-all'\n",
    "    )\n",
    "\n",
    "    Callbacks = [gpu_stats, model_ckpt, es]\n",
    "    \n",
    "    # create trainer\n",
    "    trainer = Trainer(gpus=-1, max_epochs=num_epochs, callbacks=Callbacks, logger=tb_logger)\n",
    "\n",
    "    # train model\n",
    "    trainer.fit(model=net, train_dataloader=train_dl, val_dataloaders=val_dl)\n",
    "    \n",
    "    del net\n",
    "    \n",
    "    return trainer.logged_metrics\n",
    "\n",
    "\n",
    "\n",
    "# training fn\n",
    "def train_fn(num_epochs, dataset:pd.DataFrame, n_folds=0, device='cuda'):\n",
    "    if n_folds > 0:\n",
    "        print(f'\\n[INFO] Training with {n_folds}-folds cross validation...\\n')\n",
    "        dataset = make_folds(df=dataset,stratified=False, save=False, k=n_folds)\n",
    "        losses = []\n",
    "        f1_val = []\n",
    "        f1_train = []\n",
    "\n",
    "        for fold_num in range(n_folds):\n",
    "            print(f'[INFO] Runung fold {fold_num}')\n",
    "            m = run_fold(fold_num, num_epochs, dataset)\n",
    "            # print fold results \n",
    "            best_epoch = m['epoch']\n",
    "            best_train_f1 = m['f1_train_epoch']\n",
    "            best_val_f1 = m['f1_val']\n",
    "            print(f'[INFO] Fold : {fold_num}')\n",
    "            print(f'[INFO] Best Epoch : {best_epoch}')\n",
    "            print(f'[INFO] Best Training f1 score : {best_train_f1}')\n",
    "            print(f'[INFO] Best Validation f1 score : {best_val_f1}')\n",
    "            # save fold metrics\n",
    "            f1_train.append(best_train_f1)\n",
    "            f1_val.append(best_val_f1)\n",
    "            \n",
    "    else:\n",
    "        m = run(num_epochs, dataset)\n",
    "    \n",
    "    if len(f1_val) > 0:\n",
    "        return (np.array(f1_val), np.array(f1_train), m)\n",
    "    else:\n",
    "        return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0:09<00:02,  8.33it/s]\u001b[A\n",
      "Epoch 4:  93%|| 268/287 [02:04<00:08,  2.16it/s, loss=0.280, v_num=1, f1_train_step=0.909, f1_val=0.734, f1_train_epoch=0.835]\n",
      "Validating:  80%|  | 77/96 [00:09<00:02,  8.30it/s]\u001b[A\n",
      "Epoch 4:  94%|| 270/287 [02:04<00:07,  2.17it/s, loss=0.280, v_num=1, f1_train_step=0.909, f1_val=0.734, f1_train_epoch=0.835]\n",
      "Validating:  82%| | 79/96 [00:09<00:02,  8.43it/s]\u001b[A\n",
      "Epoch 4:  95%|| 272/287 [02:04<00:06,  2.18it/s, loss=0.280, v_num=1, f1_train_step=0.909, f1_val=0.734, f1_train_epoch=0.835]\n",
      "Validating:  84%| | 81/96 [00:09<00:01,  8.29it/s]\u001b[A\n",
      "Epoch 4:  95%|| 274/287 [02:04<00:05,  2.19it/s, loss=0.280, v_num=1, f1_train_step=0.909, f1_val=0.734, f1_train_epoch=0.835]\n",
      "Validating:  86%| | 83/96 [00:10<00:01,  8.35it/s]\u001b[A\n",
      "Epoch 4:  96%|| 276/287 [02:05<00:04,  2.20it/s, loss=0.280, v_num=1, f1_train_step=0.909, f1_val=0.734, f1_train_epoch=0.835]\n",
      "Validating:  89%| | 85/96 [00:10<00:01,  7.94it/s]\u001b[A\n",
      "Epoch 4:  97%|| 278/287 [02:05<00:04,  2.22it/s, loss=0.280, v_num=1, f1_train_step=0.909, f1_val=0.734, f1_train_epoch=0.835]\n",
      "Validating:  91%| | 87/96 [00:10<00:01,  8.17it/s]\u001b[A\n",
      "Epoch 4:  98%|| 280/287 [02:05<00:03,  2.23it/s, loss=0.280, v_num=1, f1_train_step=0.909, f1_val=0.734, f1_train_epoch=0.835]\n",
      "Validating:  93%|| 89/96 [00:10<00:00,  8.14it/s]\u001b[A\n",
      "Epoch 4:  98%|| 282/287 [02:05<00:02,  2.24it/s, loss=0.280, v_num=1, f1_train_step=0.909, f1_val=0.734, f1_train_epoch=0.835]\n",
      "Validating:  95%|| 91/96 [00:11<00:00,  8.22it/s]\u001b[A\n",
      "Epoch 4:  99%|| 284/287 [02:06<00:01,  2.25it/s, loss=0.280, v_num=1, f1_train_step=0.909, f1_val=0.734, f1_train_epoch=0.835]\n",
      "Validating:  97%|| 93/96 [00:11<00:00,  8.09it/s]\u001b[A\n",
      "Epoch 4: 100%|| 286/287 [02:06<00:00,  2.26it/s, loss=0.280, v_num=1, f1_train_step=0.909, f1_val=0.734, f1_train_epoch=0.835]\n",
      "Epoch 4: 100%|| 287/287 [02:08<00:00,  2.23it/s, loss=0.280, v_num=1, f1_train_step=0.909, f1_val=0.748, f1_train_epoch=0.835]\n",
      "Epoch 5:  67%|   | 192/287 [01:54<00:56,  1.67it/s, loss=0.219, v_num=1, f1_train_step=1, f1_val=0.748, f1_train_epoch=0.875]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   1%|          | 1/96 [00:00<00:11,  8.53it/s]\u001b[A\n",
      "Epoch 5:  68%|   | 194/287 [01:55<00:55,  1.68it/s, loss=0.219, v_num=1, f1_train_step=1, f1_val=0.748, f1_train_epoch=0.875]\n",
      "Validating:   3%|         | 3/96 [00:00<00:10,  8.46it/s]\u001b[A\n",
      "Epoch 5:  68%|   | 196/287 [01:55<00:53,  1.70it/s, loss=0.219, v_num=1, f1_train_step=1, f1_val=0.748, f1_train_epoch=0.875]\n",
      "Validating:   5%|         | 5/96 [00:00<00:10,  8.40it/s]\u001b[A\n",
      "Epoch 5:  69%|   | 198/287 [01:55<00:51,  1.71it/s, loss=0.219, v_num=1, f1_train_step=1, f1_val=0.748, f1_train_epoch=0.875]\n",
      "Validating:   7%|         | 7/96 [00:00<00:11,  7.98it/s]\u001b[A\n",
      "Epoch 5:  70%|   | 200/287 [01:55<00:50,  1.73it/s, loss=0.219, v_num=1, f1_train_step=1, f1_val=0.748, f1_train_epoch=0.875]\n",
      "Validating:   9%|         | 9/96 [00:01<00:10,  8.08it/s]\u001b[A\n",
      "Epoch 5:  70%|   | 202/287 [01:56<00:48,  1.74it/s, loss=0.219, v_num=1, f1_train_step=1, f1_val=0.748, f1_train_epoch=0.875]\n",
      "Validating:  11%|        | 11/96 [00:01<00:10,  8.15it/s]\u001b[A\n",
      "Epoch 5:  71%|   | 204/287 [01:56<00:47,  1.75it/s, loss=0.219, v_num=1, f1_train_step=1, f1_val=0.748, f1_train_epoch=0.875]\n",
      "Validating:  14%|        | 13/96 [00:01<00:10,  8.19it/s]\u001b[A\n",
      "Epoch 5:  72%|  | 206/287 [01:56<00:45,  1.77it/s, loss=0.219, v_num=1, f1_train_step=1, f1_val=0.748, f1_train_epoch=0.875]\n",
      "Validating:  16%|        | 15/96 [00:01<00:10,  8.03it/s]\u001b[A\n",
      "Epoch 5:  72%|  | 208/287 [01:56<00:44,  1.78it/s, loss=0.219, v_num=1, f1_train_step=1, f1_val=0.748, f1_train_epoch=0.875]\n",
      "Validating:  18%|        | 17/96 [00:02<00:09,  8.16it/s]\u001b[A\n",
      "Epoch 5:  73%|  | 210/287 [01:57<00:42,  1.79it/s, loss=0.219, v_num=1, f1_train_step=1, f1_val=0.748, f1_train_epoch=0.875]\n",
      "Validating:  20%|        | 19/96 [00:02<00:09,  8.10it/s]\u001b[A\n",
      "Epoch 5:  74%|  | 212/287 [01:57<00:41,  1.81it/s, loss=0.219, v_num=1, f1_train_step=1, f1_val=0.748, f1_train_epoch=0.875]\n",
      "Validating:  22%|       | 21/96 [00:02<00:09,  8.23it/s]\u001b[A\n",
      "Epoch 5:  75%|  | 214/287 [01:57<00:40,  1.82it/s, loss=0.219, v_num=1, f1_train_step=1, f1_val=0.748, f1_train_epoch=0.875]\n",
      "Validating:  24%|       | 23/96 [00:02<00:08,  8.13it/s]\u001b[A\n",
      "Epoch 5:  75%|  | 216/287 [01:57<00:38,  1.83it/s, loss=0.219, v_num=1, f1_train_step=1, f1_val=0.748, f1_train_epoch=0.875]\n",
      "Validating:  26%|       | 25/96 [00:03<00:08,  8.29it/s]\u001b[A\n",
      "Epoch 5:  76%|  | 218/287 [01:58<00:37,  1.85it/s, loss=0.219, v_num=1, f1_train_step=1, f1_val=0.748, f1_train_epoch=0.875]\n",
      "Validating:  28%|       | 27/96 [00:03<00:08,  8.36it/s]\u001b[A\n",
      "Epoch 5:  77%|  | 220/287 [01:58<00:36,  1.86it/s, loss=0.219, v_num=1, f1_train_step=1, f1_val=0.748, f1_train_epoch=0.875]\n",
      "Validating:  30%|       | 29/96 [00:03<00:08,  8.36it/s]\u001b[A\n",
      "Epoch 5:  77%|  | 222/287 [01:58<00:34,  1.87it/s, loss=0.219, v_num=1, f1_train_step=1, f1_val=0.748, f1_train_epoch=0.875]\n",
      "Validating:  32%|      | 31/96 [00:03<00:07,  8.17it/s]\u001b[A\n",
      "Epoch 5:  78%|  | 224/287 [01:58<00:33,  1.89it/s, loss=0.219, v_num=1, f1_train_step=1, f1_val=0.748, f1_train_epoch=0.875]\n",
      "Validating:  34%|      | 33/96 [00:04<00:07,  8.37it/s]\u001b[A\n",
      "Epoch 5:  79%|  | 226/287 [01:59<00:32,  1.90it/s, loss=0.219, v_num=1, f1_train_step=1, f1_val=0.748, f1_train_epoch=0.875]\n",
      "Validating:  36%|      | 35/96 [00:04<00:07,  8.29it/s]\u001b[A\n",
      "Epoch 5:  79%|  | 228/287 [01:59<00:30,  1.91it/s, loss=0.219, v_num=1, f1_train_step=1, f1_val=0.748, f1_train_epoch=0.875]\n",
      "Validating:  39%|      | 37/96 [00:04<00:07,  8.31it/s]\u001b[A\n",
      "Epoch 5:  80%|  | 230/287 [01:59<00:29,  1.92it/s, loss=0.219, v_num=1, f1_train_step=1, f1_val=0.748, f1_train_epoch=0.875]\n",
      "Validating:  41%|      | 39/96 [00:04<00:07,  7.93it/s]\u001b[A\n",
      "Epoch 5:  81%|  | 232/287 [01:59<00:28,  1.94it/s, loss=0.219, v_num=1, f1_train_step=1, f1_val=0.748, f1_train_epoch=0.875]\n",
      "Validating:  43%|     | 41/96 [00:05<00:06,  8.10it/s]\u001b[A\n",
      "Epoch 5:  82%| | 234/287 [02:00<00:27,  1.95it/s, loss=0.219, v_num=1, f1_train_step=1, f1_val=0.748, f1_train_epoch=0.875]\n",
      "Validating:  45%|     | 43/96 [00:05<00:06,  8.16it/s]\u001b[A\n",
      "Epoch 5:  82%| | 236/287 [02:00<00:25,  1.96it/s, loss=0.219, v_num=1, f1_train_step=1, f1_val=0.748, f1_train_epoch=0.875]\n",
      "Validating:  47%|     | 45/96 [00:05<00:06,  8.22it/s]\u001b[A\n",
      "Epoch 5:  83%| | 238/287 [02:00<00:24,  1.97it/s, loss=0.219, v_num=1, f1_train_step=1, f1_val=0.748, f1_train_epoch=0.875]\n",
      "Validating:  49%|     | 47/96 [00:05<00:06,  7.51it/s]\u001b[A\n",
      "Epoch 5:  84%| | 240/287 [02:00<00:23,  1.99it/s, loss=0.219, v_num=1, f1_train_step=1, f1_val=0.748, f1_train_epoch=0.875]\n",
      "Validating:  51%|     | 49/96 [00:06<00:05,  7.92it/s]\u001b[A\n",
      "Epoch 5:  84%| | 242/287 [02:01<00:22,  2.00it/s, loss=0.219, v_num=1, f1_train_step=1, f1_val=0.748, f1_train_epoch=0.875]\n",
      "Validating:  53%|    | 51/96 [00:06<00:05,  8.11it/s]\u001b[A\n",
      "Epoch 5:  85%| | 244/287 [02:01<00:21,  2.01it/s, loss=0.219, v_num=1, f1_train_step=1, f1_val=0.748, f1_train_epoch=0.875]\n",
      "Validating:  55%|    | 53/96 [00:06<00:05,  8.19it/s]\u001b[A\n",
      "Epoch 5:  86%| | 246/287 [02:01<00:20,  2.02it/s, loss=0.219, v_num=1, f1_train_step=1, f1_val=0.748, f1_train_epoch=0.875]\n",
      "Validating:  57%|    | 55/96 [00:06<00:05,  7.49it/s]\u001b[A\n",
      "Epoch 5:  86%| | 248/287 [02:01<00:19,  2.04it/s, loss=0.219, v_num=1, f1_train_step=1, f1_val=0.748, f1_train_epoch=0.875]\n",
      "Validating:  59%|    | 57/96 [00:07<00:04,  7.84it/s]\u001b[A\n",
      "Epoch 5:  87%| | 250/287 [02:02<00:18,  2.05it/s, loss=0.219, v_num=1, f1_train_step=1, f1_val=0.748, f1_train_epoch=0.875]\n",
      "Validating:  61%|   | 59/96 [00:07<00:04,  8.23it/s]\u001b[A\n",
      "Epoch 5:  88%| | 252/287 [02:02<00:16,  2.06it/s, loss=0.219, v_num=1, f1_train_step=1, f1_val=0.748, f1_train_epoch=0.875]\n",
      "Validating:  64%|   | 61/96 [00:07<00:04,  8.34it/s]\u001b[A\n",
      "Epoch 5:  89%| | 254/287 [02:02<00:15,  2.07it/s, loss=0.219, v_num=1, f1_train_step=1, f1_val=0.748, f1_train_epoch=0.875]\n",
      "Validating:  66%|   | 63/96 [00:07<00:03,  8.31it/s]\u001b[A\n",
      "Epoch 5:  89%| | 256/287 [02:02<00:14,  2.09it/s, loss=0.219, v_num=1, f1_train_step=1, f1_val=0.748, f1_train_epoch=0.875]\n",
      "Validating:  68%|   | 65/96 [00:07<00:03,  8.34it/s]\u001b[A\n",
      "Epoch 5:  90%| | 258/287 [02:02<00:13,  2.10it/s, loss=0.219, v_num=1, f1_train_step=1, f1_val=0.748, f1_train_epoch=0.875]\n",
      "Validating:  70%|   | 67/96 [00:08<00:03,  8.32it/s]\u001b[A\n",
      "Epoch 5:  91%| | 260/287 [02:03<00:12,  2.11it/s, loss=0.219, v_num=1, f1_train_step=1, f1_val=0.748, f1_train_epoch=0.875]\n",
      "Validating:  72%|  | 69/96 [00:08<00:03,  8.30it/s]\u001b[A\n",
      "Epoch 5:  91%|| 262/287 [02:03<00:11,  2.12it/s, loss=0.219, v_num=1, f1_train_step=1, f1_val=0.748, f1_train_epoch=0.875]\n",
      "Validating:  74%|  | 71/96 [00:08<00:03,  8.24it/s]\u001b[A\n",
      "Epoch 5:  92%|| 264/287 [02:03<00:10,  2.13it/s, loss=0.219, v_num=1, f1_train_step=1, f1_val=0.748, f1_train_epoch=0.875]\n",
      "Validating:  76%|  | 73/96 [00:08<00:02,  8.26it/s]\u001b[A\n",
      "Epoch 5:  93%|| 266/287 [02:03<00:09,  2.15it/s, loss=0.219, v_num=1, f1_train_step=1, f1_val=0.748, f1_train_epoch=0.875]\n",
      "Validating:  78%|  | 75/96 [00:09<00:02,  8.36it/s]\u001b[A\n",
      "Epoch 5:  93%|| 268/287 [02:04<00:08,  2.16it/s, loss=0.219, v_num=1, f1_train_step=1, f1_val=0.748, f1_train_epoch=0.875]\n",
      "Validating:  80%|  | 77/96 [00:09<00:02,  8.36it/s]\u001b[A\n",
      "Epoch 5:  94%|| 270/287 [02:04<00:07,  2.17it/s, loss=0.219, v_num=1, f1_train_step=1, f1_val=0.748, f1_train_epoch=0.875]\n",
      "Validating:  82%| | 79/96 [00:09<00:02,  8.44it/s]\u001b[A\n",
      "Epoch 5:  95%|| 272/287 [02:04<00:06,  2.18it/s, loss=0.219, v_num=1, f1_train_step=1, f1_val=0.748, f1_train_epoch=0.875]\n",
      "Validating:  84%| | 81/96 [00:09<00:01,  7.90it/s]\u001b[A\n",
      "Epoch 5:  95%|| 274/287 [02:04<00:05,  2.19it/s, loss=0.219, v_num=1, f1_train_step=1, f1_val=0.748, f1_train_epoch=0.875]\n",
      "Validating:  86%| | 83/96 [00:10<00:01,  8.04it/s]\u001b[A\n",
      "Epoch 5:  96%|| 276/287 [02:05<00:04,  2.20it/s, loss=0.219, v_num=1, f1_train_step=1, f1_val=0.748, f1_train_epoch=0.875]\n",
      "Validating:  89%| | 85/96 [00:10<00:01,  8.26it/s]\u001b[A\n",
      "Epoch 5:  97%|| 278/287 [02:05<00:04,  2.22it/s, loss=0.219, v_num=1, f1_train_step=1, f1_val=0.748, f1_train_epoch=0.875]\n",
      "Validating:  91%| | 87/96 [00:10<00:01,  8.24it/s]\u001b[A\n",
      "Epoch 5:  98%|| 280/287 [02:05<00:03,  2.23it/s, loss=0.219, v_num=1, f1_train_step=1, f1_val=0.748, f1_train_epoch=0.875]\n",
      "Validating:  93%|| 89/96 [00:10<00:00,  8.12it/s]\u001b[A\n",
      "Epoch 5:  98%|| 282/287 [02:05<00:02,  2.24it/s, loss=0.219, v_num=1, f1_train_step=1, f1_val=0.748, f1_train_epoch=0.875]\n",
      "Validating:  95%|| 91/96 [00:11<00:00,  8.25it/s]\u001b[A\n",
      "Epoch 5:  99%|| 284/287 [02:06<00:01,  2.25it/s, loss=0.219, v_num=1, f1_train_step=1, f1_val=0.748, f1_train_epoch=0.875]\n",
      "Validating:  97%|| 93/96 [00:11<00:00,  8.32it/s]\u001b[A\n",
      "Epoch 5: 100%|| 286/287 [02:06<00:00,  2.26it/s, loss=0.219, v_num=1, f1_train_step=1, f1_val=0.748, f1_train_epoch=0.875]\n",
      "Epoch 5: 100%|| 287/287 [02:06<00:00,  2.27it/s, loss=0.219, v_num=1, f1_train_step=1, f1_val=0.739, f1_train_epoch=0.875]\n",
      "Epoch 6:  13%|        | 37/287 [00:22<02:33,  1.63it/s, loss=0.125, v_num=1, f1_train_step=0.929, f1_val=0.739, f1_train_epoch=0.909]\n",
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/zeusdric/anaconda3/envs/pytorch-env/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py\", line 494, in train\n",
      "    self.train_loop.run_training_epoch()\n",
      "  File \"/home/zeusdric/anaconda3/envs/pytorch-env/lib/python3.6/site-packages/pytorch_lightning/trainer/training_loop.py\", line 561, in run_training_epoch\n",
      "    batch_output = self.run_training_batch(batch, batch_idx, dataloader_idx)\n",
      "  File \"/home/zeusdric/anaconda3/envs/pytorch-env/lib/python3.6/site-packages/pytorch_lightning/trainer/training_loop.py\", line 728, in run_training_batch\n",
      "    self.optimizer_step(optimizer, opt_idx, batch_idx, train_step_and_backward_closure)\n",
      "  File \"/home/zeusdric/anaconda3/envs/pytorch-env/lib/python3.6/site-packages/pytorch_lightning/trainer/training_loop.py\", line 470, in optimizer_step\n",
      "    optimizer, batch_idx, opt_idx, train_step_and_backward_closure, *args, **kwargs\n",
      "  File \"/home/zeusdric/anaconda3/envs/pytorch-env/lib/python3.6/site-packages/pytorch_lightning/accelerators/accelerator.py\", line 124, in optimizer_step\n",
      "    **kwargs,\n",
      "  File \"/home/zeusdric/anaconda3/envs/pytorch-env/lib/python3.6/site-packages/pytorch_lightning/core/lightning.py\", line 1380, in optimizer_step\n",
      "    optimizer.step(closure=optimizer_closure, *args, **kwargs)\n",
      "  File \"/home/zeusdric/anaconda3/envs/pytorch-env/lib/python3.6/site-packages/torch/optim/lr_scheduler.py\", line 67, in wrapper\n",
      "    return wrapped(*args, **kwargs)\n",
      "  File \"/home/zeusdric/anaconda3/envs/pytorch-env/lib/python3.6/site-packages/transformers/optimization.py\", line 267, in step\n",
      "    loss = closure()\n",
      "  File \"/home/zeusdric/anaconda3/envs/pytorch-env/lib/python3.6/site-packages/pytorch_lightning/trainer/training_loop.py\", line 723, in train_step_and_backward_closure\n",
      "    self.trainer.hiddens\n",
      "  File \"/home/zeusdric/anaconda3/envs/pytorch-env/lib/python3.6/site-packages/pytorch_lightning/trainer/training_loop.py\", line 813, in training_step_and_backward\n",
      "    result = self.training_step(split_batch, batch_idx, opt_idx, hiddens)\n",
      "  File \"/home/zeusdric/anaconda3/envs/pytorch-env/lib/python3.6/site-packages/pytorch_lightning/trainer/training_loop.py\", line 320, in training_step\n",
      "    training_step_output = self.trainer.accelerator_backend.training_step(args)\n",
      "  File \"/home/zeusdric/anaconda3/envs/pytorch-env/lib/python3.6/site-packages/pytorch_lightning/accelerators/gpu_accelerator.py\", line 72, in training_step\n",
      "    output = self.__training_step(args)\n",
      "  File \"/home/zeusdric/anaconda3/envs/pytorch-env/lib/python3.6/site-packages/pytorch_lightning/accelerators/gpu_accelerator.py\", line 80, in __training_step\n",
      "    output = self.trainer.model.training_step(*args)\n",
      "  File \"<ipython-input-19-a1eb8b305b47>\", line 91, in training_step\n",
      "    f1_score = f1(preds = preds, target=targets, num_classes=self.num_classes)\n",
      "  File \"/home/zeusdric/anaconda3/envs/pytorch-env/lib/python3.6/site-packages/pytorch_lightning/metrics/functional/f_beta.py\", line 155, in f1\n",
      "    return fbeta(preds, target, num_classes, 1.0, threshold, average, multilabel)\n",
      "  File \"/home/zeusdric/anaconda3/envs/pytorch-env/lib/python3.6/site-packages/pytorch_lightning/metrics/functional/f_beta.py\", line 107, in fbeta\n",
      "    return _fbeta_compute(true_positives, predicted_positives, actual_positives, beta, average)\n",
      "  File \"/home/zeusdric/anaconda3/envs/pytorch-env/lib/python3.6/site-packages/pytorch_lightning/metrics/functional/f_beta.py\", line 54, in _fbeta_compute\n",
      "    return class_reduce(num, denom, weights=actual_positives, class_reduction=average)\n",
      "  File \"/home/zeusdric/anaconda3/envs/pytorch-env/lib/python3.6/site-packages/pytorch_lightning/metrics/functional/reduction.py\", line 70, in class_reduce\n",
      "    fraction[fraction != fraction] = 0\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/zeusdric/anaconda3/envs/pytorch-env/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3343, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-23-801b685f3250>\", line 1, in <module>\n",
      "    m = train_fn(num_epochs=config.num_epochs, n_folds=config.n_folds, dataset=train_df)\n",
      "  File \"<ipython-input-20-81520957f5e8>\", line 147, in train_fn\n",
      "    m = run_fold(fold_num, num_epochs, dataset)\n",
      "  File \"<ipython-input-20-81520957f5e8>\", line 65, in run_fold\n",
      "    trainer.fit(model=net, train_dataloader=train_dl, val_dataloaders=val_dl)\n",
      "  File \"/home/zeusdric/anaconda3/envs/pytorch-env/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py\", line 445, in fit\n",
      "    results = self.accelerator_backend.train()\n",
      "  File \"/home/zeusdric/anaconda3/envs/pytorch-env/lib/python3.6/site-packages/pytorch_lightning/accelerators/gpu_accelerator.py\", line 64, in train\n",
      "    results = self.train_or_test()\n",
      "  File \"/home/zeusdric/anaconda3/envs/pytorch-env/lib/python3.6/site-packages/pytorch_lightning/accelerators/accelerator.py\", line 66, in train_or_test\n",
      "    results = self.trainer.train()\n",
      "  File \"/home/zeusdric/anaconda3/envs/pytorch-env/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py\", line 533, in train\n",
      "    self.train_loop.on_train_end()\n",
      "  File \"/home/zeusdric/anaconda3/envs/pytorch-env/lib/python3.6/site-packages/pytorch_lightning/trainer/training_loop.py\", line 210, in on_train_end\n",
      "    model.cpu()\n",
      "  File \"/home/zeusdric/anaconda3/envs/pytorch-env/lib/python3.6/site-packages/pytorch_lightning/utilities/device_dtype_mixin.py\", line 132, in cpu\n",
      "    return super().cpu()\n",
      "  File \"/home/zeusdric/anaconda3/envs/pytorch-env/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 471, in cpu\n",
      "    return self._apply(lambda t: t.cpu())\n",
      "  File \"/home/zeusdric/anaconda3/envs/pytorch-env/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 359, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/home/zeusdric/anaconda3/envs/pytorch-env/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 359, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/home/zeusdric/anaconda3/envs/pytorch-env/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 359, in _apply\n",
      "    module._apply(fn)\n",
      "  [Previous line repeated 3 more times]\n",
      "  File \"/home/zeusdric/anaconda3/envs/pytorch-env/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 392, in _apply\n",
      "    grad_applied = fn(param.grad)\n",
      "  File \"/home/zeusdric/anaconda3/envs/pytorch-env/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 471, in <lambda>\n",
      "    return self._apply(lambda t: t.cpu())\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/zeusdric/anaconda3/envs/pytorch-env/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2044, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/zeusdric/anaconda3/envs/pytorch-env/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 1169, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/home/zeusdric/anaconda3/envs/pytorch-env/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 316, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/zeusdric/anaconda3/envs/pytorch-env/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 350, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/home/zeusdric/anaconda3/envs/pytorch-env/lib/python3.6/inspect.py\", line 1490, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/home/zeusdric/anaconda3/envs/pytorch-env/lib/python3.6/inspect.py\", line 1448, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/home/zeusdric/anaconda3/envs/pytorch-env/lib/python3.6/inspect.py\", line 693, in getsourcefile\n",
      "    if os.path.exists(filename):\n",
      "  File \"/home/zeusdric/anaconda3/envs/pytorch-env/lib/python3.6/genericpath.py\", line 19, in exists\n",
      "    os.stat(path)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'NoneType' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/pytorch-env/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    493\u001b[0m                 \u001b[0;31m# run train epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 494\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_loop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_training_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    495\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch-env/lib/python3.6/site-packages/pytorch_lightning/trainer/training_loop.py\u001b[0m in \u001b[0;36mrun_training_epoch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    560\u001b[0m             \u001b[0;31m# ------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m             \u001b[0mbatch_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_training_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch-env/lib/python3.6/site-packages/pytorch_lightning/trainer/training_loop.py\u001b[0m in \u001b[0;36mrun_training_batch\u001b[0;34m(self, batch, batch_idx, dataloader_idx)\u001b[0m\n\u001b[1;32m    727\u001b[0m                         \u001b[0;31m# optimizer step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 728\u001b[0;31m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_step_and_backward_closure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    729\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch-env/lib/python3.6/site-packages/pytorch_lightning/trainer/training_loop.py\u001b[0m in \u001b[0;36moptimizer_step\u001b[0;34m(self, optimizer, opt_idx, batch_idx, train_step_and_backward_closure, *args, **kwargs)\u001b[0m\n\u001b[1;32m    469\u001b[0m             self.trainer.accelerator_backend.optimizer_step(\n\u001b[0;32m--> 470\u001b[0;31m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_step_and_backward_closure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    471\u001b[0m             )\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch-env/lib/python3.6/site-packages/pytorch_lightning/accelerators/accelerator.py\u001b[0m in \u001b[0;36moptimizer_step\u001b[0;34m(self, optimizer, batch_idx, opt_idx, lambda_closure, *args, **kwargs)\u001b[0m\n\u001b[1;32m    123\u001b[0m             \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m             \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m         )\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch-env/lib/python3.6/site-packages/pytorch_lightning/core/lightning.py\u001b[0m in \u001b[0;36moptimizer_step\u001b[0;34m(self, epoch, batch_idx, optimizer, optimizer_idx, optimizer_closure, on_tpu, using_native_amp, using_lbfgs, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1379\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1380\u001b[0;31m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclosure\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer_closure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1381\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch-env/lib/python3.6/site-packages/torch/optim/lr_scheduler.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     66\u001b[0m                 \u001b[0mwrapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch-env/lib/python3.6/site-packages/transformers/optimization.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    266\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mclosure\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 267\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclosure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    268\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch-env/lib/python3.6/site-packages/pytorch_lightning/trainer/training_loop.py\u001b[0m in \u001b[0;36mtrain_step_and_backward_closure\u001b[0;34m()\u001b[0m\n\u001b[1;32m    722\u001b[0m                                 \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 723\u001b[0;31m                                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhiddens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    724\u001b[0m                             )\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch-env/lib/python3.6/site-packages/pytorch_lightning/trainer/training_loop.py\u001b[0m in \u001b[0;36mtraining_step_and_backward\u001b[0;34m(self, split_batch, batch_idx, opt_idx, optimizer, hiddens)\u001b[0m\n\u001b[1;32m    812\u001b[0m         \u001b[0;31m# lightning module hook\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 813\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhiddens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    814\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_curr_step_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch-env/lib/python3.6/site-packages/pytorch_lightning/trainer/training_loop.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, split_batch, batch_idx, opt_idx, hiddens)\u001b[0m\n\u001b[1;32m    319\u001b[0m             \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_train_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhiddens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m             \u001b[0mtraining_step_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    321\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_training_step_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_step_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch-env/lib/python3.6/site-packages/pytorch_lightning/accelerators/gpu_accelerator.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__training_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch-env/lib/python3.6/site-packages/pytorch_lightning/accelerators/gpu_accelerator.py\u001b[0m in \u001b[0;36m__training_step\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-a1eb8b305b47>\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, batch, batch_idx)\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;31m# f1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m         \u001b[0mf1_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch-env/lib/python3.6/site-packages/pytorch_lightning/metrics/functional/f_beta.py\u001b[0m in \u001b[0;36mf1\u001b[0;34m(preds, target, num_classes, beta, threshold, average, multilabel)\u001b[0m\n\u001b[1;32m    154\u001b[0m     \"\"\"\n\u001b[0;32m--> 155\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfbeta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmultilabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/pytorch-env/lib/python3.6/site-packages/pytorch_lightning/metrics/functional/f_beta.py\u001b[0m in \u001b[0;36mfbeta\u001b[0;34m(preds, target, num_classes, beta, threshold, average, multilabel)\u001b[0m\n\u001b[1;32m    106\u001b[0m     )\n\u001b[0;32m--> 107\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_fbeta_compute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrue_positives\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted_positives\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactual_positives\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch-env/lib/python3.6/site-packages/pytorch_lightning/metrics/functional/f_beta.py\u001b[0m in \u001b[0;36m_fbeta_compute\u001b[0;34m(true_positives, predicted_positives, actual_positives, beta, average)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbeta\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mprecision\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrecall\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mclass_reduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdenom\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mactual_positives\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_reduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maverage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch-env/lib/python3.6/site-packages/pytorch_lightning/metrics/functional/reduction.py\u001b[0m in \u001b[0;36mclass_reduce\u001b[0;34m(num, denom, weights, class_reduction)\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0;31m# for some (or all) classes which will produce nans\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m     \u001b[0mfraction\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfraction\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mfraction\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "\u001b[0;32m<ipython-input-23-801b685f3250>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_folds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_folds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-20-81520957f5e8>\u001b[0m in \u001b[0;36mtrain_fn\u001b[0;34m(num_epochs, dataset, n_folds, device)\u001b[0m\n\u001b[1;32m    146\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'[INFO] Runung fold {fold_num}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m             \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_fold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfold_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m             \u001b[0;31m# print fold results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-20-81520957f5e8>\u001b[0m in \u001b[0;36mrun_fold\u001b[0;34m(fold_num, num_epochs, dataset)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_dl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_dl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch-env/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, train_dataloader, val_dataloaders, datamodule)\u001b[0m\n\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 445\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    446\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mteardown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch-env/lib/python3.6/site-packages/pytorch_lightning/accelerators/gpu_accelerator.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;31m# train or test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_or_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch-env/lib/python3.6/site-packages/pytorch_lightning/accelerators/accelerator.py\u001b[0m in \u001b[0;36mtrain_or_test\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m             \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch-env/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    532\u001b[0m                 \u001b[0;31m# hook\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 533\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_loop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    534\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch-env/lib/python3.6/site-packages/pytorch_lightning/trainer/training_loop.py\u001b[0m in \u001b[0;36mon_train_end\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    209\u001b[0m             \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch-env/lib/python3.6/site-packages/pytorch_lightning/utilities/device_dtype_mixin.py\u001b[0m in \u001b[0;36mcpu\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    131\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__update_properties\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch-env/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mcpu\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    470\u001b[0m         \"\"\"\n\u001b[0;32m--> 471\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    472\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch-env/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    358\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 359\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch-env/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    358\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 359\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch-env/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    358\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 359\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch-env/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    358\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 359\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch-env/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    358\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 359\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch-env/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    358\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 359\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch-env/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    391\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 392\u001b[0;31m                         \u001b[0mgrad_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    393\u001b[0m                     \u001b[0mshould_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch-env/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    470\u001b[0m         \"\"\"\n\u001b[0;32m--> 471\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    472\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/pytorch-env/lib/python3.6/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2043\u001b[0m                         \u001b[0;31m# in the engines. This should return a list of strings.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2044\u001b[0;31m                         \u001b[0mstb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render_traceback_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2045\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'KeyboardInterrupt' object has no attribute '_render_traceback_'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch-env/lib/python3.6/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2045\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2046\u001b[0m                         stb = self.InteractiveTB.structured_traceback(etype,\n\u001b[0;32m-> 2047\u001b[0;31m                                             value, tb, tb_offset=tb_offset)\n\u001b[0m\u001b[1;32m   2048\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2049\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_showtraceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch-env/lib/python3.6/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1434\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1435\u001b[0m         return FormattedTB.structured_traceback(\n\u001b[0;32m-> 1436\u001b[0;31m             self, etype, value, tb, tb_offset, number_of_lines_of_context)\n\u001b[0m\u001b[1;32m   1437\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch-env/lib/python3.6/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1334\u001b[0m             \u001b[0;31m# Verbose modes need a full traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1335\u001b[0m             return VerboseTB.structured_traceback(\n\u001b[0;32m-> 1336\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb_offset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_of_lines_of_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1337\u001b[0m             )\n\u001b[1;32m   1338\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'Minimal'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch-env/lib/python3.6/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n\u001b[0;32m-> 1193\u001b[0;31m                                                                tb_offset)\n\u001b[0m\u001b[1;32m   1194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1195\u001b[0m         \u001b[0mcolors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mColors\u001b[0m  \u001b[0;31m# just a shorthand + quicker name lookup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch-env/lib/python3.6/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mformat_exception_as_a_whole\u001b[0;34m(self, etype, evalue, etb, number_of_lines_of_context, tb_offset)\u001b[0m\n\u001b[1;32m   1148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1150\u001b[0;31m         \u001b[0mlast_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_recursion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_etype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1152\u001b[0m         \u001b[0mframes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_records\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch-env/lib/python3.6/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mfind_recursion\u001b[0;34m(etype, value, records)\u001b[0m\n\u001b[1;32m    449\u001b[0m     \u001b[0;31m# first frame (from in to out) that looks different.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_recursion_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 451\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[0;31m# Select filename, lineno, func_name to track frames with\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
     ]
    }
   ],
   "source": [
    "m = train_fn(num_epochs=config.num_epochs, n_folds=config.n_folds, dataset=train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls ../logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_v0(ckpth_path):\n",
    "    if len(ckpth_path.split('v0')) == 1:\n",
    "        \n",
    "        \n",
    "        ckpth_path = ckpth_path.split('.')\n",
    "        ckpth_path.insert(1, \"-v0.\")\n",
    "        ckpth_path = ''.join(ckpth_path)\n",
    "        return ckpth_path    \n",
    "    \n",
    "    else:\n",
    "        pass\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir ../logs/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  target\n",
       "0   0       0\n",
       "1   2       0\n",
       "2   3       0\n",
       "3   9       0\n",
       "4  11       0"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Heard about #earthquake is different cities, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text\n",
       "0   0     NaN      NaN                 Just happened a terrible car crash\n",
       "1   2     NaN      NaN  Heard about #earthquake is different cities, s...\n",
       "2   3     NaN      NaN  there is a forest fire at spot pond, geese are...\n",
       "3   9     NaN      NaN           Apocalypse lighting. #Spokane #wildfires\n",
       "4  11     NaN      NaN      Typhoon Soudelor kills 28 in China and Taiwan"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disasterTweetsClassifier-all-v0.ckpt\n"
     ]
    }
   ],
   "source": [
    "! ls ../working/models/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['disasterTweetsClassifier-fold-4-v0.ckpt',\n",
       " 'disasterTweetsClassifier-fold-1.ckpt',\n",
       " 'disasterTweetsClassifier-fold-3.ckpt',\n",
       " 'disasterTweetsClassifier-fold-2-v0.ckpt',\n",
       " 'disasterTweetsClassifier-fold-0.ckpt']"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir('../working/models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def make_prediction(ckpt_dir, ids, attention_mask, k=0):\n",
    "    if k > 0:\n",
    "        preds = []\n",
    "        models = os.listdir(ckpt_dir)\n",
    "        \n",
    "        for md in tqdm(models, position=0):\n",
    "            ckpt = th.load(os.path.join(ckpt_dir, md))\n",
    "        \n",
    "            new_model = DisasterTweetsClassifier(num_epochs=config.num_epochs,\n",
    "                                             lr=config.lr, \n",
    "                                             freeze=False,\n",
    "                                             num_classes=config.num_classes,\n",
    "                                             dropout = .3,\n",
    "                                             len_train_ds=len(train_ds))    \n",
    "\n",
    "            # load model weights\n",
    "            new_model.load_state_dict(ckpt['state_dict'])\n",
    "            new_model = new_model.to(\"cpu\")\n",
    "            new_model.eval()\n",
    "\n",
    "            # predict\n",
    "            pred =  new_model(ids=ids, attention_mask=attention_mask).detach().numpy().flatten()[0]\n",
    "            preds.append(pred)\n",
    "\n",
    "        pred = np.array(preds).mean()\n",
    "    else:\n",
    "        ckpt = th.load(ckpt_path)\n",
    "        new_model = DisasterTweetsClassifier(num_epochs=config.num_epochs,\n",
    "                                         lr=config.lr, \n",
    "                                         freeze=False,\n",
    "                                         num_classes=config.num_classes,\n",
    "                                         dropout = .3,\n",
    "                                         len_train_ds=len(train_ds))    \n",
    "\n",
    "        # load model weights\n",
    "        new_model.load_state_dict(ckpt['state_dict'])\n",
    "        new_model = new_model.to(\"cpu\")\n",
    "        new_model.eval()\n",
    "\n",
    "        # predict\n",
    "        pred =  new_model(ids=ids, attention_mask=attention_mask).detach().numpy().flatten()[0]\n",
    "    \n",
    "    return pred\n",
    "\n",
    "\n",
    "def decode(pred):\n",
    "    if pred >=.7:\n",
    "        target = 1\n",
    "    else:\n",
    "        target = 0\n",
    "        \n",
    "    return target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_submission(batch_size=32):\n",
    "    th.cuda.empty_cache()\n",
    "    test_ds = DisasterDataset(df=test_df, max_len = config.max_len, task='test')\n",
    "    test_dl =  DataLoader(dataset=test_ds, \n",
    "                          batch_size=batch_size, \n",
    "                          shuffle=False)\n",
    "\n",
    "    total_preds = []\n",
    "\n",
    "    for model in os.listdir(config.models_dir):\n",
    "        print(model + ' is predicting on batch \\n')\n",
    "        ckpt_path =( os.path.join(config.models_dir, model))\n",
    "        ckpt = th.load(ckpt_path)\n",
    "        new_model = DisasterTweetsClassifier(num_epochs=config.num_epochs,\n",
    "                                         lr=config.lr, \n",
    "                                         freeze=False,\n",
    "                                         num_classes=config.num_classes,\n",
    "                                         dropout = .3,\n",
    "                                         len_train_ds=len(train_ds))    \n",
    "\n",
    "        # load model weights\n",
    "        new_model.load_state_dict(ckpt['state_dict'])\n",
    "        new_model = new_model.to(\"cuda\")\n",
    "        new_model.eval()\n",
    "\n",
    "        model_preds = []\n",
    "\n",
    "        for bath_idx, batch in enumerate(tqdm(test_dl, position=0, desc='Predicting on test batch')) :\n",
    "            preds = new_model(ids = batch['ids'].squeeze(1).to(\"cuda\"), attention_mask=batch['attention_mask'].squeeze(1).to('cuda') )\n",
    "            model_preds += preds.detach().cpu().numpy().flatten().tolist()\n",
    "\n",
    "            del preds\n",
    "\n",
    "        total_preds.append(model_preds)\n",
    "        \n",
    "        \n",
    "    final_preds = np.array(total_preds).transpose().mean(axis=1)\n",
    "    \n",
    "    return [int(x > .70) for x in final_preds.tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disasterTweetsClassifier-fold-4-v0.ckptis predicting on batch \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting on test batch: 100%|| 102/102 [00:09<00:00, 10.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disasterTweetsClassifier-fold-1.ckptis predicting on batch \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting on test batch: 100%|| 102/102 [00:10<00:00, 10.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disasterTweetsClassifier-fold-3.ckptis predicting on batch \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting on test batch: 100%|| 102/102 [00:09<00:00, 10.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disasterTweetsClassifier-fold-2-v0.ckptis predicting on batch \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting on test batch: 100%|| 102/102 [00:10<00:00, 10.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disasterTweetsClassifier-fold-0.ckptis predicting on batch \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting on test batch: 100%|| 102/102 [00:09<00:00, 10.43it/s]\n"
     ]
    }
   ],
   "source": [
    "predictions = make_submission(batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  target\n",
       "0   0       0\n",
       "1   2       0\n",
       "2   3       0\n",
       "3   9       0\n",
       "4  11       0"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  target\n",
       "0   0       0\n",
       "1   2       1\n",
       "2   3       1\n",
       "3   9       1\n",
       "4  11       1"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission_df['target'] = predictions\n",
    "submission_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = vars(config)\n",
    "fn = \"\"\n",
    "for v in keys:\n",
    "    if( '__' not in v and 'path' not in v and 'dir' not in v):\n",
    "        fn+=f'{v}_{keys[v]}_'\n",
    "        \n",
    "\n",
    "submission_df.to_csv(os.path.join(config.working_dir, fn+'.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__notebook_source__.ipynb\n",
      "dataset_10_folds.csv\n",
      "logs\n",
      "max_len_115_base_model_distilbert-base-uncased_train_batch_size_32_test_batch_size_16_lr_0.0003_num_epochs_10_n_folds_5_seed_value_2021_num_classes_1_num_workers_2_test_size_0.1_.csv\n",
      "models\n"
     ]
    }
   ],
   "source": [
    "!ls ../working/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  target\n",
       "0   0       0\n",
       "1   2       1\n",
       "2   3       1\n",
       "3   9       1\n",
       "4  11       1"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(os.path.join(config.working_dir, fn+'.csv')).head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-kernel",
   "language": "python",
   "name": "pytorch-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
